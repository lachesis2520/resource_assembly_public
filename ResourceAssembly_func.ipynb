{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import integrate\n",
    "import time\n",
    "import copy\n",
    "from functools import partial\n",
    "import itertools\n",
    "import pickle\n",
    "from numpy.random import dirichlet\n",
    "import numbers\n",
    "\n",
    "try:\n",
    "    import cvxpy as cvx\n",
    "    cvxpy_installed = True\n",
    "except:\n",
    "    print('cvxpy not installed. Community.SteadyState() not available.')\n",
    "    cvxpy_installed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors={};\n",
    "colors[0] = 'k';\n",
    "colors[1] = (0,0.6,0.1);\n",
    "colors[2] = (0.3,0.6,0.1);\n",
    "colors[3] = (0.8,0.4,0.1);\n",
    "colors[4] = (0.6,0.1,1.);\n",
    "colors[5] = (0.45,0.1,1.);\n",
    "colors[6] = (0.3,0.1,1.);\n",
    "colors[7] = (0.15,0.1,1.);\n",
    "colors[8] = (0.0,0.1,1.);\n",
    "colors[9] = (0.0,0.1,0.85);\n",
    "colors[10] = (0.,0.1,0.7);\n",
    "colors[11] = (0.,0.1,0.55);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex optimization - frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of Convex optimization used in this code comes from \n",
    "**[The Community Simulator: A Python package for microbial ecology](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0230430)**\n",
    "Marsland R, Cui W, Goldford J, Mehta P (2020) The Community Simulator: A Python package for microbial ecology. PLOS ONE 15(3): e0230430.\n",
    "[https://doi.org/10.1371/journal.pone.0230430](https://doi.org/10.1371/journal.pone.0230430)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFAULT PARAMETERS FOR CONSUMER AND METABOLIC MATRICES, AND INITIAL STATE\n",
    "a_default = {'sampling':'Shuffle', #{'Gaussian','Binary','Gamma'} specifies choice of sampling algorithm\n",
    "          'q': 0.0, #Preference strength of specialist families (0 for generalist and 1 for specialist)\n",
    "          'tau' : 50, # resource supply rate\n",
    "          'R0_food':10, #unperturbed fixed point for supplied food\n",
    "          'Sgen': 0, #Number of generalist species (unbiased sampling over alll resource classes)\n",
    "          'c1':1., #Specific consumption rate in binary model\n",
    "          'supply':'external', #resource supply (see dRdt)\n",
    "          'n_wells':10, #Number of independent wells\n",
    "          'norm': 0, # growth rate tradeoff as a normalization of c.\n",
    "             \n",
    "        ### model-dependent variables\n",
    "          'regulation':'independent', #metabolic regulation (see dRdt)\n",
    "          'response':'type I', #functional response (see dRdt)\n",
    "          'l':0., #Leakage fraction\n",
    "          'p_cf' : 0.0, # CF probability\n",
    "             \n",
    "        ### Species & Resource number dependent variables\n",
    "          'S':100, #Number of species per well (randomly sampled from the pool of size Stot = sum(SA) + Sgen)\n",
    "          'SA': 60*np.ones(3), #Number of species in each specialist family (here, 3 families of 60 species)\n",
    "          'MA': 60*np.ones(3), #Number of resources in each class \n",
    "          'muc': 10, #Mean sum of consumption rates (used in all models)\n",
    "             \n",
    "        ### irrelevant variables\n",
    "          'sigc': 0, #Standard deviation of sum of consumption rates for Gaussian and Gamma models\n",
    "          'c0':0., #Sum of background consumption rates in binary model\n",
    "          'fs':0., #Fraction of secretion flux with same resource type\n",
    "          'fw':0., #Fraction of secretion flux to 'waste' resource\n",
    "          'sparsity':0., #Effective sparsity of metabolic matrix (between 0 and 1)\n",
    "          'food':0 #index of food source (when a single resource is supplied externally)\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I'm commenting the options that I do not use. \n",
    "def MakeResourceDynamics(assumptions):\n",
    "    \"\"\"\n",
    "    Construct resource dynamics. 'assumptions' must be a dictionary containing at least\n",
    "    three entries:\n",
    "    \n",
    "    response = {'type I', 'type II', 'type III'} specifies nonlinearity of growth law\n",
    "    \n",
    "    regulation = {'independent','energy','mass'} allows microbes to adjust uptake\n",
    "        rates to favor the most abundant accessible resources (measured either by\n",
    "        energy or mass)\n",
    "    \n",
    "    supply = {'off','external','self-renewing'} sets choice of\n",
    "        intrinsic resource dynamics\n",
    "        \n",
    "    Returns a function of N, R, and the model parameters, which itself returns the\n",
    "        vector of resource rates of change dR/dt\n",
    "    \"\"\"\n",
    "    sigma = {'type I': lambda R,params: params['c']*R,\n",
    "             #'type II': lambda R,params: params['c']*R/(1+params['c']*R/params['K']),\n",
    "             'type II': lambda R,params: params['c']*R/(params['cc']+R)\n",
    "             #'type III': lambda R,params: (params['c']*R)**params['n']/(1+(params['c']*R)**params['n']/params['sigma_max'])\n",
    "                     }\n",
    "    \n",
    "    u = {'independent': lambda x,params: 1.,\n",
    "         'energy': lambda x,params: (((params['w']*x)**params['nreg']).T\n",
    "                                      /np.sum((params['w']*x)**params['nreg'],axis=1)).T,\n",
    "         'mass': lambda x,params: ((x**params['nreg']).T/np.sum(x**params['nreg'],axis=1)).T\n",
    "        }\n",
    "    \n",
    "    h = {'external': lambda R,params: (params['R0']-R)/params['tau'],\n",
    "    }\n",
    "    \n",
    "    J_in = lambda R,params: (u[assumptions['regulation']](params['c']*R,params)\n",
    "                             *params['w']*sigma[assumptions['response']](R,params))\n",
    "    J_out = lambda R,params: (params['l']*J_in(R,params)).dot(params['D'].T)\n",
    "    \n",
    "    return lambda N,R,params: (h[assumptions['supply']](R,params)\n",
    "                               -(J_in(R,params)/params['w']).T.dot(N)\n",
    "                               +(J_out(R,params)/params['w']).T.dot(N))\n",
    "\n",
    "def MakeConsumerDynamics(assumptions):\n",
    "    \"\"\"\n",
    "    Construct resource dynamics. 'assumptions' must be a dictionary containing at least\n",
    "    three entries:\n",
    "    \n",
    "    response = {'type I', 'type II', 'type III'} specifies nonlinearity of growth law\n",
    "    \n",
    "    regulation = {'independent','energy','mass'} allows microbes to adjust uptake\n",
    "        rates to favor the most abundant accessible resources (measured either by\n",
    "        energy or mass)\n",
    "    \n",
    "    supply = {'off','external','self-renewing','predator'} sets choice of\n",
    "        intrinsic resource dynamics\n",
    "        \n",
    "    Returns a function of N, R, and the model parameters, which itself returns the\n",
    "        vector of consumer rates of change dN/dt\n",
    "    \"\"\"\n",
    "    sigma = {'type I': lambda R,params: params['c']*R,\n",
    "             'type II': lambda R,params: params['c']*R/(params['cc']+R)\n",
    "            }\n",
    "    \n",
    "    u = {'independent': lambda x,params: 1.,\n",
    "         'energy': lambda x,params: (((params['w']*x)**params['nreg']).T\n",
    "                                      /np.sum((params['w']*x)**params['nreg'],axis=1)).T,\n",
    "         'mass': lambda x,params: ((x**params['nreg']).T/np.sum(x**params['nreg'],axis=1)).T\n",
    "        }\n",
    "    \n",
    "    J_in = lambda R,params: (u[assumptions['regulation']](params['c']*R,params)\n",
    "                             *params['w']*sigma[assumptions['response']](R,params))\n",
    "    J_growth = lambda R,params: (1-params['l'])*J_in(R,params)\n",
    "    \n",
    "    return lambda N,R,params: params['g']*N*(np.sum(J_growth(R,params),axis=1)-params['m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##define the parameter sampling here\n",
    "def MakeMatrices(assumptions):\n",
    "    \"\"\"\n",
    "    Construct consumer matrix and metabolic matrix.\n",
    "    \n",
    "    assumptions = dictionary of metaparameters\n",
    "        'sampling' = {'Gaussian','Binary','Gamma'} specifies choice of sampling algorithm\n",
    "        'SA' = number of species in each family\n",
    "        'MA' = number of resources of each type\n",
    "        'Sgen' = number of generalist species\n",
    "        'muc' = mean sum of consumption rates\n",
    "        'sigc' = standard deviation for Gaussian sampling of consumer matrix\n",
    "        'q' = family preference strength (from 0 to 1)\n",
    "        'c0' = row sum of background consumption rates for Binary sampling\n",
    "        'c1' = specific consumption rate for Binary sampling\n",
    "        'fs' = fraction of secretion flux into same resource type\n",
    "        'fw' = fraction of secretion flux into waste resource type\n",
    "        'sparsity' = effective sparsity of metabolic matrix (from 0 to 1)\n",
    "        'wate_type' = index of resource type to designate as \"waste\"\n",
    "    \n",
    "    Returns:\n",
    "    c = consumer matrix\n",
    "    D = metabolic matrix\n",
    "    \"\"\"\n",
    "    #PREPARE VARIABLES\n",
    "    #Force number of species to be an array:\n",
    "    if isinstance(assumptions['MA'],numbers.Number):\n",
    "        assumptions['MA'] = [assumptions['MA']]\n",
    "    if isinstance(assumptions['SA'],numbers.Number):\n",
    "        assumptions['SA'] = [assumptions['SA']]\n",
    "    #Force numbers of species to be integers:\n",
    "    assumptions['MA'] = np.asarray(assumptions['MA'],dtype=int)\n",
    "    assumptions['SA'] = np.asarray(assumptions['SA'],dtype=int)\n",
    "    assumptions['Sgen'] = int(assumptions['Sgen'])\n",
    "    #Default waste type is last type in list:\n",
    "    if 'waste_type' not in assumptions.keys():\n",
    "        assumptions['waste_type']=len(assumptions['MA'])-1\n",
    "\n",
    "    #Extract total numbers of resources, consumers, resource types, and consumer families:\n",
    "    M = np.sum(assumptions['MA'])\n",
    "    T = len(assumptions['MA'])\n",
    "    S = np.sum(assumptions['SA'])+assumptions['Sgen']\n",
    "    F = len(assumptions['SA'])\n",
    "    M_waste = assumptions['MA'][assumptions['waste_type']]\n",
    "    #Construct lists of names of resources, consumers, resource types, and consumer families:\n",
    "    resource_names = ['R'+str(k) for k in range(M)]\n",
    "    type_names = ['T'+str(k) for k in range(T)]\n",
    "    family_names = ['F'+str(k) for k in range(F)]\n",
    "    consumer_names = ['S'+str(k) for k in range(S)]\n",
    "    waste_name = type_names[assumptions['waste_type']]\n",
    "    resource_index = [[type_names[m] for m in range(T) for k in range(assumptions['MA'][m])],\n",
    "                      resource_names]\n",
    "    consumer_index = [[family_names[m] for m in range(F) for k in range(assumptions['SA'][m])]\n",
    "                      +['GEN' for k in range(assumptions['Sgen'])],consumer_names]\n",
    "    \n",
    "    #PERFORM GAUSSIAN SAMPLING\n",
    "    if assumptions['sampling'] == 'Gaussian':\n",
    "        # sample from shuffled \n",
    "        c = pd.DataFrame(np.zeros((S,M)),columns=resource_index,index=consumer_index);\n",
    "        cc = pd.DataFrame(np.zeros((S,M)),columns=resource_index,index=consumer_index);\n",
    "        ## new addition: build Kij as c\n",
    "        # = pd.DataFrame(np.random.normal(1,0.2, size=(S,M)),columns=resource_index,index=consumer_index);\n",
    "        for k in range(F):\n",
    "            for j in range(T):\n",
    "                rs = (np.arange(assumptions['SA'][k]+1)[1:])*2/assumptions['SA'][k];\n",
    "                dumi1 = np.zeros((assumptions['SA'][k],assumptions['MA'][j]));\n",
    "                dumi2 = np.zeros((assumptions['SA'][k],assumptions['MA'][j]));\n",
    "                for j2 in range(assumptions['MA'][j]):\n",
    "                    dumi1[:,j2] = np.random.normal(1,0.2,size=rs.shape);\n",
    "                    dumi2[:,j2] = np.random.normal(1,0.2,size=rs.shape);\n",
    "                c.loc['F'+str(k)]['T'+str(j)] = c.loc['F'+str(k)]['T'+str(j)].values +dumi1*(dumi1>0);\n",
    "                cc.loc['F'+str(k)]['T'+str(j)] = cc.loc['F'+str(k)]['T'+str(j)].values +dumi2;\n",
    "                \n",
    "    #PERFORM Geometric SAMPLING\n",
    "    elif assumptions['sampling'] == 'Exponential':\n",
    "        # sample from shuffled \n",
    "        c = pd.DataFrame(np.zeros((S,M)),columns=resource_index,index=consumer_index);\n",
    "        cc = pd.DataFrame(np.zeros((S,M)),columns=resource_index,index=consumer_index);\n",
    "        ## new addition: build Kij as c\n",
    "        # = pd.DataFrame(np.random.normal(1,0.2, size=(S,M)),columns=resource_index,index=consumer_index);\n",
    "        for k in range(F):\n",
    "            for j in range(T):\n",
    "                rs = (np.arange(assumptions['SA'][k]+1)[1:])*2/assumptions['SA'][k];\n",
    "                dumi1 = np.zeros((assumptions['SA'][k],assumptions['MA'][j]));\n",
    "                dumi2 = np.zeros((assumptions['SA'][k],assumptions['MA'][j]));\n",
    "                for j2 in range(assumptions['MA'][j]):\n",
    "                    dumi1[:,j2] = np.random.exponential(1,size=rs.shape);\n",
    "                    dumi2[:,j2] = np.random.normal(1,0.2,size=rs.shape);\n",
    "                c.loc['F'+str(k)]['T'+str(j)] = c.loc['F'+str(k)]['T'+str(j)].values +dumi1;\n",
    "                cc.loc['F'+str(k)]['T'+str(j)] = cc.loc['F'+str(k)]['T'+str(j)].values +dumi2;\n",
    "                                \n",
    "    elif assumptions['sampling'] == 'Shuffle':\n",
    "        # sample from shuffled \n",
    "        c = pd.DataFrame(np.zeros((S,M)),columns=resource_index,index=consumer_index);\n",
    "        cc = pd.DataFrame(np.zeros((S,M)),columns=resource_index,index=consumer_index);\n",
    "        ## new addition: build Kij as c\n",
    "        for k in range(F):\n",
    "            for j in range(T):\n",
    "                rs = (np.arange(assumptions['SA'][k]+1)[1:])*2/assumptions['SA'][k];\n",
    "                dumi1 = np.zeros((assumptions['SA'][k],assumptions['MA'][j]));\n",
    "                dumi2 = np.zeros((assumptions['SA'][k],assumptions['MA'][j]));\n",
    "                for j2 in range(assumptions['MA'][j]):\n",
    "                    dumi1[:,j2] = np.random.permutation(rs);\n",
    "                    dumi2[:,j2] = np.random.normal(1,0.2,size=rs.shape);\n",
    "                c.loc['F'+str(k)]['T'+str(j)] = c.loc['F'+str(k)]['T'+str(j)].values +dumi1;\n",
    "                cc.loc['F'+str(k)]['T'+str(j)] = cc.loc['F'+str(k)]['T'+str(j)].values +dumi2;\n",
    "            \n",
    "    elif assumptions['sampling'] == 'BinaryShuffle':\n",
    "        # sample from shuffled \n",
    "        c = pd.DataFrame(np.zeros((S,M)),columns=resource_index,index=consumer_index);\n",
    "        cc = pd.DataFrame(np.zeros((S,M)),columns=resource_index,index=consumer_index);\n",
    "        ## new addition: build Kij as c\n",
    "        for k in range(F):\n",
    "            for j in range(T):\n",
    "                rs = (np.arange(assumptions['SA'][k]+1)[1:])*2/assumptions['SA'][k];\n",
    "                dumi1 = np.zeros((assumptions['SA'][k],assumptions['MA'][j]));\n",
    "                dumi2 = np.zeros((assumptions['SA'][k],assumptions['MA'][j]));\n",
    "                dumi3 = np.zeros((assumptions['SA'][k],assumptions['MA'][j]));\n",
    "                for j2 in range(assumptions['MA'][j]):\n",
    "                    dumi1[:,j2] = np.random.permutation(rs);\n",
    "                    dumi2[:,j2] = np.random.normal(1,0.2,size=rs.shape);\n",
    "                    dumi3[:,j2] = np.random.random(size=rs.shape)>assumptions['p'];\n",
    "                dumi3[np.arange(dumi3.shape[0]), np.random.randint(dumi3.shape[1], size=dumi3.shape[0])]=1;\n",
    "                c.loc['F'+str(k)]['T'+str(j)] = c.loc['F'+str(k)]['T'+str(j)].values +dumi1*dumi3;\n",
    "                cc.loc['F'+str(k)]['T'+str(j)] = cc.loc['F'+str(k)]['T'+str(j)].values +dumi2;\n",
    "           \n",
    "    else:\n",
    "        print('Invalid distribution choice. Valid choices are kind=Gaussian, kind=Binary, kind=Gamma, kind=Uniform.')\n",
    "        return 'Error'\n",
    "    \n",
    "    \n",
    "    #metabolic matrix from binomial distribution\n",
    "    DTa = np.ones((M,M));\n",
    "    if(assumptions['p_cf']!=0):\n",
    "        DTa = np.zeros((M,M));\n",
    "        for i in np.arange(M-1)+1:\n",
    "            DTa[i,:i] = np.random.binomial(1, p=assumptions['p_cf'], size=i); #one-way CF\n",
    "            #DTa[i] = np.random.binomial(1, p=assumptions['p_cf'], size=M)/M; # all-way CF\n",
    "            DTa[i,i] = 0;\n",
    "    DT = pd.DataFrame(DTa,columns=resource_index,index=resource_index);  \n",
    "        \n",
    "    if(assumptions['norm']==1):\n",
    "        c = (c.T/np.sum(c.T)).T\n",
    "    elif(assumptions['norm']==2):\n",
    "        c = (c.T/np.sum( (c*c).T)**0.5 ).T\n",
    "        \n",
    "    return c, cc, DT.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liebig's Model of the Minimum definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_LLM(R,params):\n",
    "    return params['c']*R/(params['cc']+R)\n",
    "def dNdt_LLM(N,R,params):\n",
    "    return N*(np.min(sigma_LLM(R,params),axis=1)-params['m'])\n",
    "def dRdt_LLM(N,R,params):\n",
    "    return (params['R0']-R)*params['m'] - (params['cc']/params['c']).T.dot(N*np.min(sigma_LLM(R,params),axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembly rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembly rule simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default, with convex optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottomUp(init_state, dynamics, params, tol=1e-7, max_iters=1000):\n",
    "    # init_state is the initial state for the full complex community\n",
    "    # simulate all the pairs for each well in the initial state.\n",
    "    # return the species fractions\n",
    "    \n",
    "    S = int(np.sum(init_state[0])[0]); nsim = len( np.sum(init_state[0]) );\n",
    "    R = len( params['R0']);\n",
    "    init_inds = np.where( init_state[0].T>0 )[1].reshape(nsim,S)\n",
    "    pairs = list(itertools.combinations(np.arange(S), 2)); npair = len(pairs);\n",
    "    surv = np.zeros( (nsim, npair+1, S) );\n",
    "    growth = np.zeros(( nsim, S, R ));\n",
    "    complex_plate = Community(init_state,dynamics,params,parallel=False);\n",
    "    complex_plate.SteadyState();\n",
    "    for k in np.arange(nsim):\n",
    "        for j in np.arange(S):\n",
    "            growth[k,j,:] = np.array(params['c'])[ init_inds[k,j ], : ];\n",
    "            surv[k,-1,j] = np.array(complex_plate.N)[ init_inds[k,j], k ];\n",
    "        if(np.sum(surv[k,-1])>0): surv[k,-1] = surv[k,-1]/np.sum(surv[k,-1]);\n",
    "    for i in np.arange(npair):\n",
    "        pair_state = 0*init_state[0].copy(); np_pair_state = np.array(pair_state);\n",
    "        pair_inds = init_inds[:,pairs][:,i,:];\n",
    "        np_pair_state[pair_inds.reshape(2*nsim), np.repeat( np.arange(nsim), 2 )]=1;\n",
    "        pair_state = np_pair_state;\n",
    "        pair_plate = Community( (pair_state, init_state[1]),dynamics,params,parallel=False );\n",
    "        pair_plate.SteadyState();\n",
    "        for k in np.arange(nsim):\n",
    "            for j in np.arange(S):\n",
    "                surv[k,i,j] = np.array(pair_plate.N)[ init_inds[k,j], k ];\n",
    "            if(np.sum(surv[k,i])>0): surv[k,i] = surv[k,i]/np.sum(surv[k,i]);\n",
    "    return surv, growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottomUp_propagate(init_state, dynamics, params):\n",
    "    # init_state is the initial state for the full complex community\n",
    "    # simulate all the pairs for each well in the initial state.\n",
    "    # return the species fractions\n",
    "    \n",
    "    S = int(np.sum(init_state[0])[0]); nsim = len( np.sum(init_state[0]) );\n",
    "    R = len( params['R0']);\n",
    "    init_inds = np.where( init_state[0].T>0 )[1].reshape(nsim,S)\n",
    "    pairs = list(itertools.combinations(np.arange(S), 2)); npair = len(pairs);\n",
    "    surv = np.zeros( (nsim, npair+1, S) );\n",
    "    growth = np.zeros(( nsim, S, R ));\n",
    "    complex_plate = Community(init_state,dynamics,params,parallel=False);\n",
    "    complex_plate.Propagate(T=1e3, compress_species=False);\n",
    "    for k in np.arange(nsim):\n",
    "        for j in np.arange(S):\n",
    "            growth[k,j,:] = np.array(params['c'])[ init_inds[k,j ], : ];\n",
    "            surv[k,-1,j] = np.array(complex_plate.N)[ init_inds[k,j], k ];\n",
    "        if(np.sum(surv[k,-1])>0): surv[k,-1] = surv[k,-1]/np.sum(surv[k,-1]);\n",
    "    for i in np.arange(npair):\n",
    "        ind1 = pairs[i][0]; ind2 = pairs[i][1];\n",
    "        pair_state = 0*init_state[0].copy(); np_pair_state = np.array(pair_state);\n",
    "        pair_inds = init_inds[:,pairs][:,i,:];\n",
    "        np_pair_state[pair_inds.reshape(2*nsim), np.repeat( np.arange(nsim), 2 )]=1;\n",
    "        pair_state = np_pair_state;\n",
    "        pair_plate = Community( (pair_state, init_state[1]),dynamics,params,parallel=False );\n",
    "        pair_plate.Propagate(T=1e3, compress_species=False);\n",
    "        for k in np.arange(nsim):\n",
    "            surv[k,i,ind1] = np.array(pair_plate.N)[ init_inds[k,ind1], k ];\n",
    "            surv[k,i,ind2] = np.array(pair_plate.N)[ init_inds[k,ind2], k ];\n",
    "            if(np.sum(surv[k,i])>0): \n",
    "                surv[k,i] = surv[k,i]/np.sum(surv[k,i]);\n",
    "    return surv, growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bistability, without convex optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottomUp_b(d, R, RA, c0=[1,1,1], tmax=1000, nstep=10000):\n",
    "    t = np.linspace(0,tmax,nstep); \n",
    "    surv = np.zeros( (10, 3) );\n",
    "    nc0s = np.zeros((10,3));\n",
    "    nc0s[0]=[0.99,0.01,0]; nc0s[1]=[0.01,0.99,0];nc0s[2]=[0.01,0.,0.99];nc0s[3]=[0.99,0.,0.01];nc0s[4]=[0.,0.99,0.01];nc0s[5]=[0,0.01,0.99];nc0s[6]=[0.99,0.005,0.005];nc0s[7]=[0.005,0.99,0.005];nc0s[8]=[0.005,0.005,0.99];nc0s[9]=[0.33,0.33,0.33];\n",
    "    for i in np.arange(10):\n",
    "        nc0=np.array([nc0s[i,0],nc0s[i,1],nc0s[i,2],c0[0], c0[1], c0[2] ]);\n",
    "        nc = integrate.odeint(dnc33, nc0, t, args= (d,R,RA,c0) );\n",
    "        if(np.sum(nc[-1,:3]>0 )):\n",
    "            surv[i] = nc[-1,:3]/np.sum(nc[-1,:3]);\n",
    "    return surv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembly rule predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arule_h( surv, repeat=True, method=0, thres=1e-4 ):\n",
    "    S = surv.shape[-1]; nsim = surv.shape[0];\n",
    "    pred = np.zeros((nsim, S));\n",
    "    fracs = surv / np.sum(surv, axis=-1, keepdims=True);\n",
    "    for k in np.arange(nsim):\n",
    "        for i in np.arange(S):\n",
    "            ind = fracs[k,:-1,i]>0;\n",
    "            if(method==0):\n",
    "                pred[k,i] = 1/( np.sum(1/fracs[k,:-1,i][ind]-1) +1);\n",
    "            elif(method==1):\n",
    "                pred[k,i] = 1/np.average(1/fracs[k,:-1,i][ind]);\n",
    "            elif(method==2):\n",
    "                pred[k,i] = (np.prod(fracs[k,:-1,i][ind]))**(1/len(ind));\n",
    "            elif(method==3):\n",
    "                pred[k,i] = np.average(fracs[k,:-1,i][ind]);\n",
    "            elif(method==4):\n",
    "                pred[k,i] = (np.prod(fracs[k,:-1,i][ind]))**(1/len(ind));\n",
    "        if(method==4):\n",
    "            dumi = np.copy(pred[k]);\n",
    "            for i in np.arange(S):\n",
    "                ind = fracs[k,:-1,i]>0;\n",
    "                pred[k,i] = (np.prod( fracs[k,:-1,i][ind]**dumi[ind] ))**(1/ np.sum(dumi[ind]) );\n",
    "    pred = pred/np.sum(pred, axis=-1, keepdims=True);\n",
    "    if(repeat):\n",
    "        pairs = list(itertools.combinations(np.arange(S),2));\n",
    "        surv = surv>thres;\n",
    "        pairs_pool = np.zeros((len(pairs), S));\n",
    "        for i in np.arange(len(pairs)):\n",
    "            pairs_pool[i, pairs[i][0]]=1; pairs_pool[i, pairs[i][1]]=1;  \n",
    "        fracs0 = np.zeros((nsim, S, S)); # pairwise outcome\n",
    "        for i in np.arange(len(pairs)):\n",
    "            fracs0[:, pairs[i][0], pairs[i][1] ] = fracs[:,i,pairs[i][0]];\n",
    "            fracs0[:, pairs[i][1], pairs[i][0] ] = fracs[:,i,pairs[i][1]];\n",
    "        for k in ( np.arange(nsim)):\n",
    "            cnt=1; \n",
    "            while( cnt>0 ):\n",
    "                ind1 = np.where(pred[k]<thres)[0];\n",
    "                ind2 = list( np.where(pred[k]>=thres)[0] );\n",
    "                cnt=0;\n",
    "                ind3 = [];\n",
    "                for i in ind1:\n",
    "                    if( np.all(fracs0[k,i,ind2]>thres) ):\n",
    "                        cnt+=1;\n",
    "                        ind3 = ind3 + [i];\n",
    "                for i in np.array(ind3):\n",
    "                    fracs0[k,i,i] = 1;\n",
    "                    if( np.all(fracs0[k,i,ind3]>thres) ):\n",
    "                        ind2 = ind2+[i];\n",
    "                    fracs0[k,i,i] = 0;\n",
    "                ind2 = np.array(ind2);\n",
    "                if(len(ind2)>1):\n",
    "                    for i in ind2:\n",
    "                        ind = ind2[ind2!=i];\n",
    "                        if(method==0):\n",
    "                            pred[k,i] = 1/( np.sum(1/fracs0[k,i,ind]-1) +1);\n",
    "                        elif(method==1):\n",
    "                            pred[k,i] = 1/np.average(1/fracs0[k,i,ind]);\n",
    "                        elif(method==2):\n",
    "                            pred[k,i] = (np.prod(fracs0[k,i,ind]))**(1/len(ind));\n",
    "                        elif(method==3):\n",
    "                            pred[k,i] = np.average(fracs0[k,i,ind])\n",
    "    pred = pred/np.sum(pred, axis=-1, keepdims=True);\n",
    "    return pred;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-species predictions with classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arule(surv, thres=1e-4):\n",
    "    surv = surv>thres;\n",
    "    nsurv = np.sum(surv[:,:3,:], axis=1); nsim = surv.shape[0]; # count pairwise competitions that each species survived\n",
    "    nsurvf = np.sum( surv[:,-1,:] , axis=1); \n",
    "    outcome = np.zeros((7,4)); #all, predicted, expected violation, unexpected violation\n",
    "    violations = np.zeros(nsim);\n",
    "    types = np.zeros((6,3)); ##types: classification of pairwise outcomes by counting each species' survival.\n",
    "    types[0]=[0,1,2]; types[1]=[1,1,1]; types[2]=[1,1,2]; types[3]=[0,2,2]; types[4]=[1,2,2]; types[5]=[2,2,2];\n",
    "    for i in np.arange(nsim):\n",
    "        if( nsurvf[i] == 0 ): continue; # bug: extinction of all species\n",
    "        for j in np.arange(6):\n",
    "            if( np.all( np.sort(nsurv[i])== types[j] ) ): break;\n",
    "        if(j==0): # 3 exclusions, one winning twice\n",
    "            outcome[j,0]+=1; pred=[0,0,0]; pred[np.argmax(nsurv[i])]=1; \n",
    "            outcome[j,1]+=np.all(surv[i,-1]==pred); \n",
    "            if( np.any(surv[i,-1]!=pred) ): \n",
    "#                print(i, pred, surv[i]);\n",
    "                outcome[j,3]+=1;\n",
    "        elif(j==1): # cycle\n",
    "            outcome[j,0]+=1; pred=[0,0,0]; outcome[j,3]+=1;\n",
    "#            print(i, pred, surv[i]);\n",
    "        elif(j==2): # one coexistence and two exclusions\n",
    "            det = np.linalg.det( surv[i,:3] ) # times that each species survived\n",
    "            \n",
    "            if(det==0): # one species won in both of two exclusions\n",
    "                outcome[j+1,0]+=1; pred=[0,0,0]; pred[np.argmax( np.sum(surv[i,:3], axis=0) ) ]=1;\n",
    "                outcome[j+1,1]+=np.all(surv[i,-1]==pred);\n",
    "                if( np.any(surv[i,-1]!=pred) ): \n",
    "#                    print(i, pred, surv[i]);\n",
    "                    outcome[j,3]+=1;\n",
    "            else: # A->B, B->C, C-><-A\n",
    "                outcome[j,0]+=1; pred = surv[ i, np.argmax( np.sum(surv[i,:3], axis=1) )  ];\n",
    "                outcome[j,1]+=np.all(surv[i,-1]==pred);\n",
    "                if( np.any(surv[i,-1]!=pred) ): \n",
    "#                    print(i, pred, surv[i]);\n",
    "                    outcome[j,3]+=1;\n",
    "                \n",
    "            #if(np.sum(surv[i,winner])==1):\n",
    "            #    outcome[j+1,0]+=1; pred = surv[ i, np.argmax( np.sum(surv[i,:3], axis=0) )  ];\n",
    "            #    outcome[j+1,1]+=np.all(surv[i,-1]==pred);\n",
    "            #    if( np.any(surv[i,-1]!=pred) ): print(i, pred, surv[i])\n",
    "            #elif(np.sum(surv[i,winner])==2):\n",
    "            #    outcome[j,0]+=1; pred=[0,0,0]; #pred[winner]=1; \n",
    "            #    pred = surv[ i, np.argmax( np.sum(surv[i,:3], axis=0) )  ];\n",
    "            #    outcome[j,1]+=np.all(surv[i,-1]==pred); \n",
    "            #    if( np.any(surv[i,-1]!=pred) ): print(i, surv[i])\n",
    "        elif(j==3): #one species lost both exclusions, two winners coexist\n",
    "            outcome[j+1,0] += 1; pred=[1,1,1]; pred[np.argmin(nsurv[i])]=0; outcome[j+1,1]+=np.all(surv[i,-1]==pred);\n",
    "            if( np.any(surv[i,-1]!=pred) ): \n",
    "#                print(i, pred, surv[i]);\n",
    "                outcome[j+1,3]+=1;\n",
    "        elif(j==4): # two coexistence and one exclusion\n",
    "            outcome[j+1,0] += 1; pred=[1,1,1]; pred[np.argmin(nsurv[i])]=0;\n",
    "            if( np.all(surv[i,-1]==pred) ):\n",
    "                outcome[j+1,1]+=1; \n",
    "            else:\n",
    "                pred2= 1 - surv[ i, np.argmin( np.sum(surv[i,:3], axis=1) )  ]; # reverse of the exclusion outcome  \n",
    "                if( np.all(surv[i,-1]==pred2) ):\n",
    "                    outcome[j+1,2]+=1; \n",
    "                else:\n",
    "#                    print(i, pred, surv[i]);\n",
    "                    outcome[j+1,3]+=1;\n",
    "        elif(j==5):\n",
    "            outcome[j+1,0] += 1; pred=[1,1,1]; \n",
    "            if( np.all(surv[i,-1]==pred) ):\n",
    "                outcome[j+1,1]+=1; \n",
    "            else:\n",
    "                if( np.any( [np.all(surv[i,-1]==[0,1,1]), np.all(surv[i,-1]==[1,0,1]), np.all(surv[i,-1]==[1,1,0]) ] ) ):\n",
    "                    outcome[j+1,2]+=1;\n",
    "                else:\n",
    "#                    print(i, pred, surv[i]);\n",
    "                    outcome[j+1,3]+=1;\n",
    "        else: \n",
    "            print(i);\n",
    "        violations[i]+=j*(1-np.all(surv[i,-1]==pred));\n",
    "    return outcome, violations;    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-species prediction with classification, with bistable pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arule_b( surv, thres=1e-2):\n",
    "    S = 3; nsim = surv.shape[0];\n",
    "    ### first, filter out bistable outcomes.\n",
    "    surv = 1*(surv>thres);\n",
    "    checks = np.zeros((nsim,3));\n",
    "    unobss = np.zeros(nsim);\n",
    "    ## 4 types of edges -> 3^4 possibilities...? very difficult to do. \n",
    "    ## instead, only consider cases where trio outcome is bistable.\n",
    "    cnt1=0; cnt2=0; cnt3=0; cnt4=0;\n",
    "    types=[];\n",
    "    ind_viol = [];\n",
    "    for k in np.arange(nsim):\n",
    "        if( np.all(surv[k,0]+surv[k,1]==np.array([1,1,0])) ): checks[k,0]=1; # pair 1 is bistable\n",
    "        if( np.all(surv[k,2]+surv[k,3]==np.array([1,0,1])) ): checks[k,1]=1; # pair 2 is bistable\n",
    "        if( np.all(surv[k,4]+surv[k,5]==np.array([0,1,1])) ): checks[k,2]=1; # pair 3 is bistable\n",
    "        if( 0 in np.sum(surv[k], axis=-1) ):\n",
    "            cnt4+=1; checks[k]+=4;  continue; #error: no one survived in some competition\n",
    "        if( np.sum(surv[k,0]+surv[k,1])%2==1 or np.sum(surv[k,2]+surv[k,3])%2==1 or np.sum(surv[k,4]+surv[k,5])%2==1): \n",
    "            cnt3+=1; checks[k]+=4; continue; #error: pairwise outcome has a bistable coexistence\n",
    "        nb = np.sum(checks[k]);\n",
    "        if(nb==0 or nb>3): continue; # find outcomes with 1-3 bistable pairs and no obvious error\n",
    "        mxw = np.argmax(np.sum(surv[k,:6],axis=0)); # species with max win\n",
    "        mnw = np.argmin(np.sum(surv[k,:6],axis=0)); # species with min win\n",
    "        ns1 = np.sum(surv[k,:6])/2 ## survival sum in all pairs\n",
    "        pred = [[0,0,0]];\n",
    "        if(nb==1):\n",
    "            ind1 = 2-np.where(checks[k]==1)[0]; \n",
    "            ### pair that does not involve ind1 is the only bistable one. \n",
    "            ### then the outcome possibilities depend on other two edges' coex/exc\n",
    "            ## exc exc 1, exc exc 2, exc exc 3, exc co 1, exc co 2, co co\n",
    "            ns2 = np.sum(surv[k,:6,ind1])/2; ## survival count of the non-bistable species\n",
    "            if( ns1 == 3): # exc exc\n",
    "                if(ns2==2): # ind1 excludes both others\n",
    "                    pred[0][mxw]=1; # \n",
    "                    types+=[1]; \n",
    "                elif(ns2==1): # ind1 is excluded by one species\n",
    "                    pred[0][mxw]=1;\n",
    "                    types+=[2];  \n",
    "                elif(ns2==0): # ind1 is excluded by both species\n",
    "                    pred[0][(mnw-1)%3]=1;\n",
    "                    pred+=[[0,0,0]];\n",
    "                    pred[1][(mnw+1)%3]=1;\n",
    "                    types+=[3];\n",
    "            elif( ns1== 4): # exc coex\n",
    "                coex = surv[k,np.argmax(np.sum(surv[k,:6], axis=-1))];\n",
    "                if(ns2==2): # ind1 excludes one, coexists with the other\n",
    "                    pred[0]=list(coex);\n",
    "                    types+=[4];\n",
    "                elif(ns2==1): # ind1 is excluded by one, coexists with the other\n",
    "                    pred[0]=list(coex);\n",
    "                    pred+=[list(1-coex)];\n",
    "                    types+=[5];\n",
    "            elif( ns1 == 5): # ind1 coexists with others\n",
    "                pred[0][mxw]=1;\n",
    "                pred[0][(mxw+1)%3]=1;\n",
    "                pred+=[[0,0,0]];\n",
    "                pred[1][mxw]=1;\n",
    "                pred[1][(mxw-1)%3]=1;\n",
    "                types+=[6];\n",
    "        elif(nb==2):\n",
    "            if(ns1==3): # bist bist exc\n",
    "                pred[0][(mnw+1)%3]=1;\n",
    "                pred+=[[0,0,0]];\n",
    "                pred[1][(mnw-1)%3]=1;\n",
    "                types+=[7];\n",
    "            elif(ns1==4): # bist bist coex\n",
    "                pred[0][mnw]=1;\n",
    "                pred+=[[1,1,1]];\n",
    "                pred[1][mnw]=0;\n",
    "                types+=[8];\n",
    "        elif(nb==3): # bist bist bist\n",
    "            pred[0]=[1,0,0];\n",
    "            pred+=[[0,1,0]];\n",
    "            pred+=[[0,0,1]];\n",
    "            types+=[9];\n",
    "    ### now pred is done! need to check whether each surv[6:] falls into one of pred. \n",
    "        trio = [];\n",
    "        for i in np.arange(4):\n",
    "            trio += [list(surv[k,6+i])];\n",
    "        viol = [x for x in trio if x not in pred];# + [x for x in pred if x not in trio];\n",
    "        unobs = [x for x in pred if x not in trio];\n",
    "        unobss[k] = len(unobs)\n",
    "        if(len(viol)==0): cnt1+=1;\n",
    "        else: \n",
    "            cnt2+=1;\n",
    "            ind_viol += [k];\n",
    "            print(surv[k], mxw,mnw,pred,ns1,nb)\n",
    "    print(cnt1, cnt2, cnt3, cnt4) # agreement, violation, bistable coex?, no survivor\n",
    "    return (np.sum(checks, axis=-1), types, ind_viol, unobss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multispecies qualitative prediction (obsolete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arule_m( surv, thres=1e-2 ): #surv = np.zeros((nsim,((S*(S-1))//2)+1,S));\n",
    "    S = surv.shape[-1]; nsim = surv.shape[0];\n",
    "    pairs = list(itertools.combinations(np.arange(S),2));\n",
    "    surv = surv>thres;\n",
    "    pairs_pool = np.zeros((len(pairs), S));\n",
    "    for i in np.arange(len(pairs)):\n",
    "        pairs_pool[i, pairs[i][0]]=1; pairs_pool[i, pairs[i][1]]=1;    \n",
    "    prediction = np.zeros(nsim);\n",
    "    for k in np.arange(nsim):\n",
    "        # two ways.\n",
    "        # 1. build up prediction by starting from the maximal survivor in pairs and adding species one by one.\n",
    "        # 2. among survivors in complex community, check if they all pairwisely coexist. among non-survivors, check if they are all excluded.\n",
    "        # Method 1.\n",
    "        #pred = np.zeros(S);\n",
    "        #survcount = np.sum(surv[k,:-1], axis=0); \n",
    "        #pred[np.argmax(survcount)]=1;\n",
    "        #for i in np.arange(S):\n",
    "        #    inds = np.where(pred == 1)[0];\n",
    "        #    check=1;\n",
    "        #    for j in inds:\n",
    "        #        ind = np.where(pairs_pool[:,i]*pairs_pool[:,j]==1)[0][0];\n",
    "        #        if( surv[k, ind, i] == 0): check=0; break;\n",
    "        #    pred[i]=check;\n",
    "        #prediction[k] =  np.sum((surv[k, -1] - pred)**2)/S ;\n",
    "        # Method 2.\n",
    "        pred = np.zeros(S);\n",
    "        survivor = np.where( surv[k,-1]==1 )[0];\n",
    "        dead = np.where( surv[k,-1]==0 )[0];\n",
    "        for i in survivor:\n",
    "            check=1;\n",
    "            for j in survivor:\n",
    "                ind = np.where(pairs_pool[:,i]*pairs_pool[:,j]==1)[0][0];\n",
    "                if( surv[k, ind, i] == 0): check=0; break;\n",
    "            pred[i] = check;\n",
    "        for i in dead:\n",
    "            check = 1;\n",
    "            for j in survivor:\n",
    "                ind = np.where(pairs_pool[:,i]*pairs_pool[:,j]==1)[0][0];\n",
    "                check = check*surv[k, ind, i];\n",
    "            pred[i] = check;\n",
    "        prediction[k] =  np.sum((surv[k, -1] - pred)**2)/S ;\n",
    "    return 1-prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex optimization - backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeInitialState(assumptions):\n",
    "    \"\"\"\n",
    "    Construct stochastically colonized initial state, at unperturbed resource fixed point.\n",
    "    \n",
    "    assumptions = dictionary of metaparameters\n",
    "        'SA' = number of species in each family\n",
    "        'MA' = number of resources of each type\n",
    "        'Sgen' = number of generalist species\n",
    "        'n_wells' = number of independent wells in the experiment\n",
    "        'S' = initial number of species per well\n",
    "        'food' = index of supplied \"food\" resource\n",
    "        'R0_food' = unperturbed fixed point for supplied food resource\n",
    "    \n",
    "    Returns:\n",
    "    N0 = initial consumer populations\n",
    "    R0 = initial resource concentrations\n",
    "    \"\"\"\n",
    "\n",
    "    #PREPARE VARIABLES\n",
    "    #Force number of species to be an array:\n",
    "    if isinstance(assumptions['MA'],numbers.Number):\n",
    "        assumptions['MA'] = [assumptions['MA']]\n",
    "    if isinstance(assumptions['SA'],numbers.Number):\n",
    "        assumptions['SA'] = [assumptions['SA']]\n",
    "    #Force numbers of species to be integers:\n",
    "    assumptions['MA'] = np.asarray(assumptions['MA'],dtype=int)\n",
    "    assumptions['SA'] = np.asarray(assumptions['SA'],dtype=int)\n",
    "    assumptions['Sgen'] = int(assumptions['Sgen'])\n",
    "\n",
    "    #Extract total numbers of resources, consumers, resource types, and consumer families:\n",
    "    M = int(np.sum(assumptions['MA']))\n",
    "    T = len(assumptions['MA'])\n",
    "    S_tot = int(np.sum(assumptions['SA'])+assumptions['Sgen'])\n",
    "    F = len(assumptions['SA'])\n",
    "    #Construct lists of names of resources, consumers, resource types, consumer families and wells:\n",
    "    resource_names = ['R'+str(k) for k in range(M)]\n",
    "    type_names = ['T'+str(k) for k in range(T)]\n",
    "    family_names = ['F'+str(k) for k in range(F)]\n",
    "    consumer_names = ['S'+str(k) for k in range(S_tot)]\n",
    "    resource_index = [[type_names[m] for m in range(T) for k in range(assumptions['MA'][m])],\n",
    "                      resource_names]\n",
    "    consumer_index = [[family_names[m] for m in range(F) for k in range(assumptions['SA'][m])]\n",
    "                      +['GEN' for k in range(assumptions['Sgen'])],consumer_names]\n",
    "    well_names = ['W'+str(k) for k in range(assumptions['n_wells'])]\n",
    "\n",
    "    R0 = assumptions['R0_food']*np.ones((M,assumptions['n_wells']))\n",
    "    N0 = np.zeros((S_tot,assumptions['n_wells']))\n",
    "    \n",
    "    #if not isinstance(assumptions['food'],int):\n",
    "    #    assert len(assumptions['food']) == assumptions['n_wells'], 'Length of food vector must equal n_wells.'\n",
    "    #    food_list = assumptions['food']\n",
    "    #else:\n",
    "    #    food_list = np.ones(assumptions['n_wells'],dtype=int)*assumptions['food']\n",
    "\n",
    "    #if not (isinstance(assumptions['R0_food'],int) or isinstance(assumptions['R0_food'],float)):\n",
    "    #    assert len(assumptions['R0_food']) == assumptions['n_wells'], 'Length of food vector must equal n_wells.'\n",
    "    #    R0_food_list = assumptions['R0_food']\n",
    "    #else:\n",
    "    #    R0_food_list = np.ones(assumptions['n_wells'],dtype=int)*assumptions['R0_food']\n",
    "\n",
    "    for k in range(assumptions['n_wells']):\n",
    "        N0[np.random.choice(S_tot,size=assumptions['S'],replace=False),k]=1.\n",
    "        #R0[food_list[k],k] = R0_food_list[k]\n",
    "\n",
    "    N0 = pd.DataFrame(N0,index=consumer_index,columns=well_names)\n",
    "    R0 = pd.DataFrame(R0,index=resource_index,columns=well_names)\n",
    "\n",
    "    return N0, R0\n",
    "\n",
    "\n",
    "def MakeParams(assumptions):\n",
    "    \"\"\"\n",
    "    Makes a dictionary of parameters, using MakeMatrices for the matrices, MakeInitialState\n",
    "    for the resource supply point, and setting everything else to 1, except l which is zero.\n",
    "    \n",
    "    Parameter values can be modified from 1 (or zero for l) by adding their name-value pairs\n",
    "    to the assumptions dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    c, cc, D = MakeMatrices(assumptions)\n",
    "    N0,R0 = MakeInitialState(assumptions)\n",
    "    #cc = c.copy();\n",
    "#    cc = cc*0+np.random.normal(1,0.2, size=cc.shape)\n",
    "    \n",
    "    if not isinstance(assumptions['food'],int) or not isinstance(assumptions['R0_food'],int):\n",
    "        params=[{'c':c,\n",
    "                 'cc':cc,\n",
    "                'm':1,\n",
    "                'w':1,\n",
    "                'D':D,\n",
    "                'g':1,\n",
    "                'l':0,\n",
    "                'R0':R0.values[:,k],\n",
    "                'tau':1,\n",
    "                'r':1,\n",
    "                'sigma_max':1,\n",
    "                'nreg':10,\n",
    "                'n':2\n",
    "                } for k in range(assumptions['n_wells'])]\n",
    "        for item in ['m','w','g','l','tau','r','sigma_max','n','nreg']:\n",
    "            if item in assumptions.keys():\n",
    "                for k in range(assumptions['n_wells']):\n",
    "                    params[k][item] = assumptions[item]\n",
    "\n",
    "    else:\n",
    "        params={'c':c,\n",
    "                'cc':cc,\n",
    "                'm':1,\n",
    "                'w':1,\n",
    "                'D':D,\n",
    "                'g':1,\n",
    "                'l':0,\n",
    "                'R0':R0.values[:,0],\n",
    "                'tau':1,\n",
    "                'r':1,\n",
    "                'sigma_max':1,\n",
    "                'nreg':10,\n",
    "                'n':2\n",
    "                }\n",
    "            \n",
    "        for item in ['m','w','g','l','tau','r','sigma_max','n','nreg']:\n",
    "            if item in assumptions.keys():\n",
    "                params[item] = assumptions[item]\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "\n",
    "def SimpleDilution(plate, f0 = 1e-3):\n",
    "    \"\"\"\n",
    "    Returns identity matrix of size plate.n_wells, scaled by f0\n",
    "    \"\"\"\n",
    "    f = f0 * np.eye(plate.n_wells)\n",
    "    return f\n",
    "\n",
    "def BinaryRandomMatrix(a,b,p):\n",
    "    \"\"\"\n",
    "    Construct binary random matrix.\n",
    "    \n",
    "    a, b = matrix dimensions\n",
    "    \n",
    "    p = probability that element equals 1 (otherwise 0)\n",
    "    \"\"\"\n",
    "    r = np.random.rand(a,b)\n",
    "    m = np.zeros((a,b))\n",
    "    m[r<p] = 1.0\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter dimensions for MicroCRM and Lotka-Volterra\n",
    "dim_default = {\n",
    "                'SxM':['c'],\n",
    "                'MxM':['Di'],#['D','Di'],\n",
    "                'SxS':['alpha'],\n",
    "                'S':['m','g','K'],\n",
    "                'M':['e','w','r','tau','R0']\n",
    "                }\n",
    "\n",
    "class Community:\n",
    "    def __init__(self,init_state,dynamics,params,dimensions=dim_default,scale=10**9,parallel=True):\n",
    "        \"\"\"\n",
    "        Initialize a new \"96-well plate\" for growing microbial communities.\n",
    "        \n",
    "        init_state = [N0,R0] where N0 and R0 are 2-D arrays specifying the \n",
    "            initial consumer and resource concentrations, respectively, in each\n",
    "            of the wells. Each species of consumer and each resource has its\n",
    "            own row, and each well has its own column. If N0 and R0 are Pandas\n",
    "            DataFrames, the row and column labels will be preserved throughout\n",
    "            all subsequent calculations. Otherwise, standard row and column\n",
    "            labels will be automatically supplied.\n",
    "        \n",
    "        dynamics = [dNdt,dRdt] where dNdt(N,R,params) and dRdt(N,R,params) are \n",
    "            vectorized functions of the consumer and resource concentrations\n",
    "            N and R for a single well. params is a Python dictionary containing\n",
    "            the parameters that required by these functions, and is passed to \n",
    "            the new plate instance in the next argument. \n",
    "            \n",
    "        params = {'name':value,...} is a Python dictionary containing names and values\n",
    "            for all parameters. Parameters that are matrices or vectors (such as the\n",
    "            consumer preference matrix) should have their dimensions recorded in the\n",
    "            next argument. This is done automatically for the parameters of the built-\n",
    "            in Microbial Consumer Resource Model, but must be done by hand for custom\n",
    "            models.\n",
    "        dimensions = {'SxM':[name1,name2,...],...} is a dictionary specifying the \n",
    "            dimensions of all the parameters. These are used for compressing \n",
    "            the parameter arrays when species or resources are extinct. See default\n",
    "            dictionary above for proper format. Allowed dimensions are SxM, SxS, \n",
    "            MxM, M and S, where M is the number of resource types and S is the number\n",
    "            of consumer species.\n",
    "            \n",
    "        scale is a conversion factor specifying the number of individual microbial \n",
    "            cells present when N = 1. It is used in the Passage method defined \n",
    "            below to perform multinomial sampling, and controls the strength\n",
    "            of population noise. \n",
    "            \n",
    "        parallel allows for disabeling parallel integration, which is currently not\n",
    "            supported for Windows machines\n",
    "        \"\"\"\n",
    "        #SAVE INITIAL STATE\n",
    "        N, R = init_state\n",
    "        if not isinstance(N, pd.DataFrame):#add labels to consumer state\n",
    "            if len(np.shape(N)) == 1:\n",
    "                N = N[:,np.newaxis]\n",
    "            column_names = ['W'+str(k) for k in range(np.shape(N)[1])]\n",
    "            species_names = ['S'+str(k) for k in range(np.shape(N)[0])]\n",
    "            N = pd.DataFrame(N,columns=column_names)\n",
    "            N.index = species_names\n",
    "        if not isinstance(R, pd.DataFrame):#add labels to resource state\n",
    "            if len(np.shape(R)) == 1:\n",
    "                R = R[:,np.newaxis]\n",
    "            resource_names = ['R'+str(k) for k in range(np.shape(R)[0])]\n",
    "            R = pd.DataFrame(R,columns=N.keys())\n",
    "            R.index = resource_names\n",
    "        self.N = N.copy()\n",
    "        self.R = R.copy()\n",
    "        self.R0 = R.copy() #(for refreshing media on passaging if \"refresh_resource\" is turned on)\n",
    "        self.S, self.n_wells = np.shape(self.N)\n",
    "        self.M = np.shape(self.R)[0]\n",
    "        \n",
    "        #SAVE DYNAMICS\n",
    "        self.dNdt, self.dRdt = dynamics\n",
    "        \n",
    "        #SAVE PARAMETERS\n",
    "        self.params = params.copy()\n",
    "        if isinstance(self.params,list): #Allow parameter file to be a list\n",
    "            assert len(self.params) == self.n_wells, 'Length of parameter list must equal n_wells.'\n",
    "            for k in range(len(self.params)):\n",
    "                for item in self.params[k]:#strip parameters from DataFrames if necessary\n",
    "                    if isinstance(self.params[k][item],pd.DataFrame):\n",
    "                        if (item != 'c' and item != 'cc'):\n",
    "                            self.params[k][item]=self.params[k][item].values.squeeze()\n",
    "                        else:\n",
    "                            self.params[k][item]=self.params[k][item].values\n",
    "                    elif isinstance(self.params[k][item],list):\n",
    "                        self.params[k][item]=np.asarray(self.params[k][item])\n",
    "                    if 'D' not in self.params[k]:#supply dummy values for D and l if D is not specified\n",
    "                        self.params[k]['D'] = np.ones((self.M,self.M))\n",
    "                        self.params[k]['l'] = 0\n",
    "                self.params[k]['S'] = self.S\n",
    "        else:\n",
    "            for item in self.params:#strip parameters from DataFrames if necessary\n",
    "                if isinstance(self.params[item],pd.DataFrame):\n",
    "                    if (item != 'c' and item != 'cc'):\n",
    "                        self.params[item]=self.params[item].values.squeeze()\n",
    "                    else:\n",
    "                        self.params[item]=self.params[item].values\n",
    "                elif isinstance(self.params[item],list):\n",
    "                    self.params[item]=np.asarray(self.params[item])\n",
    "            if 'D' not in params:#supply dummy values for D and l if D is not specified\n",
    "                self.params['D'] = np.ones((self.M,self.M))\n",
    "                self.params['l'] = 0\n",
    "            self.params['S'] = self.S\n",
    "        \n",
    "        #SAVE DIMENSIONS, SCALE AND PARALLEL\n",
    "        self.dimensions = dimensions\n",
    "        self.scale = scale\n",
    "        self.parallel = parallel\n",
    "            \n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def Reset(self,init_state):\n",
    "        \"\"\"\n",
    "        Reset plate with new initial state, keeping same parameters.\n",
    "        \"\"\"\n",
    "        self.N, self.R = init_state\n",
    "        \n",
    "        if not isinstance(self.N, pd.DataFrame):\n",
    "            column_names = ['D'+str(k) for k in range(np.shape(self.N)[1])]\n",
    "            species_names = ['S'+str(k) for k in range(np.shape(self.N)[0])]\n",
    "            self.N = pd.DataFrame(self.N,columns=column_names)\n",
    "            self.N.index = species_names\n",
    "            \n",
    "        if not isinstance(self.R, pd.DataFrame):\n",
    "            resource_names = ['R'+str(k) for k in range(np.shape(self.R)[0])]\n",
    "            self.R = pd.DataFrame(self.R,columns=self.N.keys())\n",
    "            self.R.index = resource_names\n",
    "            \n",
    "        self.R0 = self.R.copy()\n",
    "        self.S, self.n_wells = np.shape(self.N)\n",
    "        self.M = np.shape(self.R)[0]\n",
    "        \n",
    "    def dydt(self,y,t,params,S_comp):\n",
    "        \"\"\"\n",
    "        Combine N and R into a single vector with a single dynamical equation\n",
    "        \n",
    "        y = [N1,N2,N3...NS,R1,R2,R3...RM]\n",
    "        \n",
    "        t = time\n",
    "        \n",
    "        params = params to pass to dNdt,dRdt\n",
    "        \n",
    "        S_comp = number of species in compressed consumer vector\n",
    "            (with extinct species removed)\n",
    "        \"\"\"\n",
    "        return np.hstack([self.dNdt(y[:S_comp],y[S_comp:],params),\n",
    "                          self.dRdt(y[:S_comp],y[S_comp:],params)])\n",
    "    \n",
    "    def SteadyState(self,tol=1e-7,shift_size=1,alpha=0.5,\n",
    "                    eps=1e-10,R0t_0=10,max_iters=1000,verbose=False,plot=False):\n",
    "        \"\"\"\n",
    "        Find the steady state using convex optimization.\n",
    "        \n",
    "        supply = {external, self-renewing}\n",
    "        \"\"\"\n",
    "        \n",
    "        #CONSTRUCT FULL SYSTEM STATE\n",
    "        y_in = self.N.append(self.R).values\n",
    "        \n",
    "        #PACKAGE SYSTEM STATE AND PARAMETERS IN LIST OF DICTIONARIES\n",
    "        if not isinstance(self.params,list):\n",
    "            params = [self.params]*self.n_wells\n",
    "        else:\n",
    "            params = self.params\n",
    "        well_info = [{'y0':y_in[:,k],'params':params[k]} for k in range(self.n_wells)]\n",
    "        \n",
    "        #PREPARE OPTIMIZER FOR PARALLEL PROCESSING\n",
    "        OptimizeTheseWells = partial(OptimizeWell,tol=tol,alpha=alpha,\n",
    "                                     shift_size=shift_size,max_iters=max_iters,\n",
    "                                     eps=eps,R0t_0=R0t_0,verbose=verbose,dimensions=self.dimensions)\n",
    "        \n",
    "        \n",
    "        #IF PARALLEL IS DEACTIVATED, USE ORDINARY MAP\n",
    "        y_out = np.asarray(list(map(OptimizeTheseWells,well_info))).squeeze().T\n",
    "        if len(np.shape(y_out)) == 1:#handle case of single-well plate\n",
    "            y_out = y_out[:,np.newaxis]\n",
    "        \n",
    "        #UPDATE STATE VARIABLES WITH RESULTS OF OPTIMIZATION\n",
    "        self.N = pd.DataFrame(y_out[:self.S,:],\n",
    "                              index = self.N.index, columns = self.N.keys())\n",
    "        self.R = pd.DataFrame(y_out[self.S:,:],\n",
    "                              index = self.R.index, columns = self.R.keys())\n",
    "\n",
    "        #PRINT DIAGNOSTICS\n",
    "        dNdt_f = np.asarray(list(map(self.dNdt,self.N.T.values,self.R.T.values,params)))\n",
    "        dRdt_f = np.asarray(list(map(self.dRdt,self.N.T.values,self.R.T.values,params)))\n",
    "        \n",
    "        if plot:\n",
    "            dNdt_f = np.asarray(list(map(self.dNdt,self.N.T.values,self.R.T.values,params))).reshape(-1)\n",
    "            dRdt_f = np.asarray(list(map(self.dRdt,self.N.T.values,self.R.T.values,params))).reshape(-1)\n",
    "            N = self.N.values.reshape(-1)\n",
    "            R = self.R.values.reshape(-1)\n",
    "    \n",
    "            fig,ax = plt.subplots()\n",
    "            ax.plot(dNdt_f[N>0]/N[N>0],'o',markersize=1)\n",
    "            ax.set_ylabel('Per-Capita Growth Rate')\n",
    "            ax.set_title('Consumers')\n",
    "            plt.show()\n",
    "            \n",
    "            fig,ax = plt.subplots()\n",
    "            ax.plot(dRdt_f/R,'o',markersize=1)\n",
    "            ax.set_ylabel('Per-Capita Growth Rate')\n",
    "            ax.set_title('Resources')\n",
    "            plt.show()\n",
    "            \n",
    "    def Propagate(self,T,compress_resources=False,compress_species=True):\n",
    "        \"\"\"\n",
    "        Propagate the state variables forward in time according to dNdt, dRdt.\n",
    "\n",
    "        T = time interval for propagation\n",
    "\n",
    "        compress_resources specifies whether zero-abundance resources should be\n",
    "            ignored during the propagation. This makes sense when the resources\n",
    "            are non-renewable.\n",
    "\n",
    "        compress_species specifies whether zero-abundance species should be\n",
    "            ignored during the propagation. This always makes sense for the\n",
    "            models we consider. But for user-defined models with new parameter\n",
    "            names, it must be turned off, since the package does not know how\n",
    "            to compress the parameter matrices properly.\n",
    "        \"\"\"\n",
    "        #CONSTRUCT FULL SYSTEM STATE\n",
    "        y_in = self.N.append(self.R).values\n",
    "\n",
    "        #PACKAGE SYSTEM STATE AND PARAMETERS IN LIST OF DICTIONARIES\n",
    "        if isinstance(self.params,list):\n",
    "            well_info = [{'y0':y_in[:,k],'params':self.params[k]} for k in range(self.n_wells)]\n",
    "        else:\n",
    "            well_info = [{'y0':y_in[:,k],'params':self.params} for k in range(self.n_wells)]\n",
    "\n",
    "        #PREPARE INTEGRATOR FOR PARALLEL PROCESSING\n",
    "        IntegrateTheseWells = partial(IntegrateWell,self,T=T,compress_resources=compress_resources,\n",
    "                                      compress_species=compress_species)\n",
    "\n",
    "        #INITIALIZE PARALLEL POOL AND SEND EACH WELL TO ITS OWN WORKER\n",
    "        if self.parallel:\n",
    "            pool = Pool()\n",
    "            y_out = np.asarray(pool.map(IntegrateTheseWells,well_info)).squeeze().T\n",
    "            pool.close()\n",
    "        else:\n",
    "            y_out = np.asarray(list(map(IntegrateTheseWells,well_info))).squeeze().T\n",
    "\n",
    "        #HANDLE CASE OF A SINGLE-WELL PLATE\n",
    "        if len(np.shape(y_out)) == 1:\n",
    "            y_out = y_out[:,np.newaxis]\n",
    "\n",
    "        #UPDATE STATE VARIABLES WITH RESULTS OF INTEGRATION\n",
    "        self.N = pd.DataFrame(y_out[:self.S,:],\n",
    "                              index = self.N.index, columns = self.N.keys())\n",
    "        self.R = pd.DataFrame(y_out[self.S:,:],\n",
    "                              index = self.R.index, columns = self.R.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompressParams(not_extinct_consumers,not_extinct_resources,params,dimensions,S,M):\n",
    "    params_comp = params.copy()\n",
    "    if 'SxM' in dimensions.keys():\n",
    "        for item in dimensions['SxM']:\n",
    "            if item in params_comp.keys():\n",
    "                assert np.shape(params_comp[item])==(S,M), 'Invalid shape for ' + item + '. Please update dimensions dictionary with correct dimensions.'\n",
    "                params_comp[item]=params_comp[item][not_extinct_consumers,:]\n",
    "                params_comp[item]=params_comp[item][:,not_extinct_resources]\n",
    "    if 'MxM' in dimensions.keys():\n",
    "        for item in dimensions['MxM']:\n",
    "            if item in params_comp.keys():\n",
    "                assert np.shape(params_comp[item])==(M,M), 'Invalid shape for ' + item + '. Please update dimensions dictionary with correct dimensions.'\n",
    "                params_comp[item]=params_comp[item][not_extinct_resources,:]\n",
    "                params_comp[item]=params_comp[item][:,not_extinct_resources]\n",
    "    if 'SxS' in dimensions.keys():\n",
    "        for item in dimensions['SxS']:\n",
    "            if item in params_comp.keys():\n",
    "                assert np.shape(params_comp[item])==(S,S), 'Invalid shape for ' + item + '. Please update dimensions dictionary with correct dimensions.'\n",
    "                params_comp[item]=params_comp[item][not_extinct_consumers,:]\n",
    "                params_comp[item]=params_comp[item][:, not_extinct_consumers]\n",
    "    if 'S' in dimensions.keys():\n",
    "        for item in dimensions['S']:\n",
    "            if item in params_comp.keys():\n",
    "                if type(params_comp[item]) == np.ndarray:\n",
    "                    assert len(params_comp[item])==S, 'Invalid length for ' + item + '. Please update dimensions dictionary with correct dimensions.'\n",
    "                    params_comp[item]=params_comp[item][not_extinct_consumers]\n",
    "    if 'M' in dimensions.keys():\n",
    "        for item in dimensions['M']:\n",
    "            if item in params_comp.keys():\n",
    "                if type(params_comp[item]) == np.ndarray:\n",
    "                    assert len(params_comp[item])==M, 'Invalid length for ' + item + '. Please update dimensions dictionary with correct dimensions.'\n",
    "                    params_comp[item]=params_comp[item][not_extinct_resources]  \n",
    "    return params_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimizeWell(well_info,supply='external',tol=1e-7,shift_size=1,eps=1e-20,\n",
    "                 alpha=0.5,R0t_0=10,verbose=False,max_iters=1000,dimensions={}):\n",
    "    \"\"\"\n",
    "    Uses convex optimization to find the steady state of the ecological dynamics.\n",
    "    \"\"\"\n",
    "\n",
    "    assert cvxpy_installed, 'CVXPY not found. To use SteadyState(), please download and install CVXPY from www.cvxpy.org.'\n",
    "    \n",
    "    #UNPACK INPUT\n",
    "    y0 = well_info['y0'].copy()\n",
    "    S = well_info['params']['S']\n",
    "    M = len(y0)-S\n",
    "    N = y0[:S]\n",
    "    R = y0[S:]\n",
    "    \n",
    "    #COMPRESS PARAMETERS TO GET RID OF EXTINCT SPECIES\n",
    "    assert np.isnan(N).sum() == 0, 'All species must have numeric abundances (not NaN).'\n",
    "    assert np.isnan(R).sum() == 0, 'All resources must have numeric abundances (not NaN).'\n",
    "    not_extinct_consumers = N>0\n",
    "    not_extinct_resources = np.ones(len(R),dtype=bool)\n",
    "    #Compress parameters\n",
    "    params_comp = CompressParams(not_extinct_consumers,not_extinct_resources,well_info['params'],dimensions,S,M)    \n",
    "    S = len(params_comp['c'])\n",
    "    M = len(params_comp['c'].T)\n",
    "\n",
    "    \n",
    "    failed = 0\n",
    "    if np.max(params_comp['l']) != 0:\n",
    "        assert supply == 'external', 'Replenishment must be external for crossfeeding dynamics.'\n",
    "        \n",
    "        #Make Q matrix and effective weight vector\n",
    "        w_mat = np.kron(np.ones((M,1)),np.ones((1,M))*params_comp['w'])\n",
    "        Q = np.eye(M) - params_comp['l']*params_comp['D']*w_mat/(w_mat.T)\n",
    "        Qinv = np.linalg.inv(Q)\n",
    "        Qinv_aa = np.diag(Qinv)\n",
    "        w = Qinv_aa*(1-params_comp['l'])*params_comp['w']/params_comp['tau']\n",
    "        Qinv = Qinv - np.diag(Qinv_aa)\n",
    "        \n",
    "        #Construct variables for optimizer\n",
    "        G = params_comp['c']*params_comp['w']*(1-params_comp['l'])/w #Divide by w, because we will multiply by wR\n",
    "        if isinstance(params_comp['m'],np.ndarray):\n",
    "            h = params_comp['m'].reshape((S,1))\n",
    "        else:\n",
    "            h = np.ones((S,1))*params_comp['m']\n",
    "        \n",
    "        #Initialize effective resource concentrations\n",
    "        if len(np.shape(R0t_0)) == 0:\n",
    "            R0t = R0t_0*np.ones(M)\n",
    "        else:\n",
    "            R0t = R0t_0\n",
    "        \n",
    "        #Set up the loop\n",
    "        Rf = np.inf\n",
    "        Rf_old = 0\n",
    "\n",
    "        k=0\n",
    "        ncyc=0\n",
    "        Delta = 1\n",
    "        Delta_old = 1\n",
    "        while np.linalg.norm(Rf_old - Rf) > tol and k < max_iters:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "        \n",
    "                wR = cvx.Variable(shape=(M,1)) #weighted resources\n",
    "        \n",
    "                #Need to multiply by w to get properly weighted KL divergence\n",
    "                R0t = np.sqrt(R0t**2+eps)\n",
    "                wR0 = (R0t*w).reshape((M,1))\n",
    "\n",
    "                #Solve\n",
    "                obj = cvx.Minimize(cvx.sum(cvx.kl_div(wR0, wR)))\n",
    "                constraints = [G@wR <= h, wR >= 0]\n",
    "                prob = cvx.Problem(obj, constraints)\n",
    "                prob.solver_stats\n",
    "                prob.solve(solver=cvx.ECOS,abstol=1e-8,reltol=1e-8,warm_start=True,verbose=False,max_iters=5000)\n",
    "\n",
    "                #Record the results\n",
    "                Rf_old = Rf\n",
    "                Nf=constraints[0].dual_value[0:S].reshape(S)\n",
    "                Rf=wR.value.reshape(M)/w\n",
    "\n",
    "                #Update the effective resource concentrations\n",
    "                R0t_new = params_comp['R0'] + Qinv.dot((params_comp['R0']-Rf)/params_comp['tau'])*(params_comp['tau']/Qinv_aa)\n",
    "                Delta_R0t = R0t_new-R0t\n",
    "                R0t = R0t + alpha*Delta_R0t\n",
    "                \n",
    "                Delta_old = Delta\n",
    "                Delta = np.linalg.norm(Rf_old - Rf)\n",
    "                if verbose:\n",
    "                    print('Iteration: '+str(k))\n",
    "                    print('Delta: '+str(Delta))\n",
    "                    print('---------------- '+str(time.time()-start_time)[:4]+' s ----------------')\n",
    "            except:\n",
    "                #If optimization fails, try new R0t\n",
    "                shift = shift_size*np.random.randn(M)\n",
    "                R0t = np.abs(R0t + shift)\n",
    "                \n",
    "                if verbose:\n",
    "                    print('Added '+str(shift_size)+' times random numbers')\n",
    "            k+=1\n",
    "            #Check for limit cycle\n",
    "            if np.isfinite(Delta) and Delta > tol and np.abs(Delta-Delta_old) < 0.1*tol:\n",
    "                ncyc+=1\n",
    "            if ncyc > 10:\n",
    "                print('Limit cycle detected')\n",
    "                k = max_iters\n",
    "\n",
    "        if k == max_iters:\n",
    "            print('Maximum iterations exceeded. Try decreasing alpha to improve convergence.')\n",
    "            failed = 1\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('success')\n",
    "                \n",
    "    elif params_comp['l'] == 0:\n",
    "\n",
    "        G = params_comp['c']*params_comp['tau'] #Multiply by tau, because wR has tau in the denominator\n",
    "        if isinstance(params_comp['m'],np.ndarray):\n",
    "            h = params_comp['m'].reshape((S,1))\n",
    "        else:\n",
    "            h = np.ones((S,1))*params_comp['m']\n",
    "\n",
    "        wR = cvx.Variable(shape=(M,1)) #weighted resources\n",
    "\n",
    "        #Need to multiply by w to get properly weighted KL divergence\n",
    "        wR0 = (params_comp['R0']*params_comp['w']*np.ones(M)/params_comp['tau']).reshape((M,1))\n",
    "\n",
    "        #Solve\n",
    "        obj = cvx.Minimize(cvx.sum(cvx.kl_div(wR0, wR)))\n",
    "        constraints = [G@wR <= h]\n",
    "        prob = cvx.Problem(obj, constraints)\n",
    "        prob.solver_stats\n",
    "        try:\n",
    "            prob.solve(solver=cvx.ECOS,abstol=1e-8,reltol=1e-8,warm_start=True,verbose=False,max_iters=50000)\n",
    "        except:\n",
    "            failed=1;\n",
    "\n",
    "    #Record the results\n",
    "\n",
    "    if not failed:\n",
    "        Nf=constraints[0].dual_value[0:S].reshape(S)\n",
    "        Rf=wR.value.reshape(M)*params_comp['tau']/params_comp['w']\n",
    "        N_new = np.zeros(len(N))\n",
    "        R_new = np.zeros(len(R))\n",
    "        N_new[np.where(not_extinct_consumers)[0]] = Nf\n",
    "        R_new[np.where(not_extinct_resources)[0]] = Rf\n",
    "    else:\n",
    "        N_new = np.zeros(len(N))#np.nan*N\n",
    "        R_new = np.zeros(len(R))#np.nan*R\n",
    "        if verbose:\n",
    "            print('Optimization Failed.')\n",
    "            \n",
    "    return np.hstack((N_new, R_new))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IntegrateWell(CommunityInstance,well_info,T0=0,T=1,ns=2,return_all=False,log_time=False,\n",
    "                  compress_resources=False,compress_species=True):\n",
    "    \"\"\"\n",
    "        Integrator for Propagate and TestWell methods of the Community class\n",
    "        \"\"\"\n",
    "    #MAKE LOGARITHMIC TIME AXIS FOR LONG SINGLE RUNS\n",
    "    if log_time:\n",
    "        t = 10**(np.linspace(np.log10(T0),np.log10(T0+T),ns))\n",
    "    else:\n",
    "        t = np.linspace(T0,T0+T,ns)\n",
    "    \n",
    "    #UNPACK INPUT\n",
    "    y0 = well_info['y0']\n",
    "    \n",
    "    #COMPRESS STATE AND PARAMETERS TO GET RID OF EXTINCT SPECIES\n",
    "    S = well_info['params']['S']\n",
    "    M = len(y0)-S\n",
    "    not_extinct = y0>0\n",
    "    if not compress_species:\n",
    "        not_extinct[:S] = True\n",
    "    if not compress_resources:  #only compress resources if we're running non-renewable dynamics\n",
    "        not_extinct[S:] = True\n",
    "    S_comp = np.sum(not_extinct[:S]) #record the new point dividing species from resources\n",
    "    not_extinct_idx = np.where(not_extinct)[0]\n",
    "    y0_comp = y0[not_extinct]\n",
    "    not_extinct_consumers = not_extinct[:S]\n",
    "    not_extinct_resources = not_extinct[S:]\n",
    "\n",
    "    #Compress parameters\n",
    "    params_comp = CompressParams(not_extinct_consumers,not_extinct_resources,well_info['params'],CommunityInstance.dimensions,S,M)\n",
    "\n",
    "    #INTEGRATE AND RESTORE STATE VECTOR TO ORIGINAL SIZE\n",
    "    if return_all:\n",
    "        out = integrate.odeint(CommunityInstance.dydt,y0_comp,t,args=(params_comp,S_comp),mxstep=10000,atol=1e-4)\n",
    "        traj = np.zeros((np.shape(out)[0],S+M))\n",
    "        traj[:,not_extinct_idx] = out\n",
    "        return t, traj\n",
    "    else:\n",
    "        out = integrate.odeint(CommunityInstance.dydt,y0_comp,t,args=(params_comp,S_comp),mxstep=10000,atol=1e-4)[-1]\n",
    "        yf = np.zeros(len(y0))\n",
    "        yf[not_extinct_idx] = out\n",
    "        return yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestWell(self,T = 4,well_name = None,f0 = 1.,ns=100,log_time = False,T0=0,\n",
    "                 compress_resources=False,compress_species=False,show_plots=True,axs=[]):\n",
    "        \"\"\"\n",
    "        Run a single well and plot the trajectory.\n",
    "        \n",
    "        T = duration of trajectory\n",
    "        \n",
    "        well_name = label of well to run (will choose first well if \"None\")\n",
    "        \n",
    "        f0 = fraction by which to reduce initial consumer populations. This is\n",
    "            useful when running a serial transfer simulation, where the initial\n",
    "            populations for the next plate will be a small fraction of the current\n",
    "            values\n",
    "            \n",
    "        ns = number of time points to sample\n",
    "        \n",
    "        log_time allows one to use a logarithmic time axis, which is helpful if\n",
    "            the community has very fast initial transient dynamics followed by \n",
    "            a slow convergence to steady state\n",
    "            \n",
    "        compress_resources specifies whether zero-abundance resources should be\n",
    "            ignored during the propagation. This makes sense when the resources\n",
    "            are non-renewable.\n",
    "        \"\"\"\n",
    "        #EXTRACT STATE OF A SINGLE WELL\n",
    "        if well_name == None:\n",
    "            well_name = self.N.keys()[0]\n",
    "        N_well = self.N.copy()[well_name] * f0\n",
    "        R_well = self.R.copy()[well_name]\n",
    "        if isinstance(self.params,list):\n",
    "            params_well = self.params[np.where(np.asarray(self.N.keys())==well_name)[0][0]]\n",
    "        else:\n",
    "            params_well = self.params\n",
    "        \n",
    "        #INTEGRATE WELL\n",
    "        t, out = IntegrateWell(self,{'y0':N_well.append(R_well).values,'params':params_well},T=T,ns=ns,T0=T0,\n",
    "                               return_all=True,log_time=log_time,compress_resources=compress_resources,\n",
    "                               compress_species=compress_species)\n",
    "        \n",
    "        Ntraj = out[:,:self.S]\n",
    "        Rtraj = out[:,self.S:]\n",
    "        \n",
    "        #PLOT TRAJECTORY\n",
    "        if show_plots:\n",
    "            if axs == []:\n",
    "                fig, axs = plt.subplots(2,sharex=True)\n",
    "            else:\n",
    "                assert len(axs) == 2, 'Must supply two sets of axes.'\n",
    "\n",
    "            if log_time:\n",
    "                axs[0].semilogx(t,Ntraj)\n",
    "                axs[1].semilogx(t,Rtraj)\n",
    "            else:\n",
    "                axs[0].plot(t,Ntraj)\n",
    "                axs[1].plot(t,Rtraj)\n",
    "            axs[0].set_ylabel('Consumer Abundance')\n",
    "            axs[1].set_ylabel('Resource Abundance')\n",
    "            axs[1].set_xlabel('Time')\n",
    "\n",
    "        return t, Ntraj, Rtraj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous (plotting etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outcome(outcome, figsize=(5.55,2)):\n",
    "    fig, ax = plt.subplots(figsize=figsize);\n",
    "    outcome2 = np.copy(outcome);\n",
    "    # 0123456 -> 034256\n",
    "    outcome2[1] = outcome[3]; outcome2[2]=outcome[4]; outcome2[3]=outcome[2];\n",
    "    outcome2[4] = outcome[5]; outcome2[5]=outcome[6];\n",
    "    #outcome2[1:-1] = outcome[2:];\n",
    "    ax.bar( np.arange(6), outcome2[:-1,1], color='#7AC943', alpha=0.8 );\n",
    "    ax.bar( np.arange(6), outcome2[:-1,0]-outcome2[:-1,1], bottom=outcome2[:-1,1], color='#93278F', alpha=0.8);\n",
    "    labels=[];\n",
    "    for i in np.arange(6):\n",
    "        labels+= ['%i / %i' %(outcome2[i,1],outcome2[i,0])];\n",
    "    ax.set_ylim(0,150); ax.set_xticks([]);\n",
    "    ax.set_xlim(-0.5,5.5);\n",
    "    #ax.set_xlabel('3-species pairwise outcomes');\n",
    "    ax.set_ylabel('Frequency');\n",
    "    #ax.legend(['Assembly Rule prediction','Assembly Rule violation'], loc='upper right');\n",
    "    ax.set_yticks([0,100,200])\n",
    "    #ax.set_xticks(np.arange(6)); #ax.set_yticks();\n",
    "    #ax.set_xticklabels(labels)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnc33(nc, t, d, R, RA, c0):\n",
    "    n1,n2,n3,c1,c2,c3=nc;\n",
    "    return [ n1*(R[0,0]*c1+R[0,1]*c2+R[0,2]*c3 - d ), n2*(R[1,0]*c1+R[1,1]*c2+R[1,2]*c3 - d ), n3*(R[2,0]*c1+R[2,1]*c2+R[2,2]*c3 - d ), -c1*(RA[0,0]*n1+RA[1,0]*n2+RA[2,0]*n3+d)+d*c0[1],  -c2*(RA[0,1]*n1+RA[1,1]*n2+RA[2,1]*n3+d)+d*c0[1],  -c3*(RA[0,2]*n1+RA[1,2]*n2+RA[2,2]*n3+d)+d*c0[2]  ]\n",
    "def dnc2(nc, t, d, R, RA,c10,c20):\n",
    "    n1,n2,c1,c2=nc;\n",
    "    return [ n1*(R[0,0]*c1+R[0,1]*c2 - d ), n2*(R[1,0]*c1+R[1,1]*c2 - d ), -c1*(RA[0,0]*n1+RA[1,0]*n2+d)+d*c10,  -c2*(RA[0,1]*n1+RA[1,1]*n2+d)+d*c20  ]\n",
    "def dnc3(nc, t, d, R, RA,c10,c20):\n",
    "    n1,n2,n3,c1,c2=nc;\n",
    "    return [ n1*(R[0,0]*c1+R[0,1]*c2 - d ), n2*(R[1,0]*c1+R[1,1]*c2 - d ), n3*(R[2,0]*c1+R[2,1]*c2 - d ), -c1*(RA[0,0]*n1+RA[1,0]*n2+RA[2,0]*n3+d)+d*c10,  -c2*(RA[0,1]*n1+RA[1,1]*n2+RA[2,1]*n3+d)+d*c20  ]\n",
    "\n",
    "def dnc(nc, t, d, R, RA, c0):\n",
    "    ns, nr = R.shape;\n",
    "    n = nc[:ns]; c = nc[ns:];\n",
    "    dn = n*( np.sum(R*r, axis=1) - d);\n",
    "    dc = -c*( np.sum(RA.T*n, axis=1) ) + d*(c0-c);\n",
    "    return np.concatenate([dn,dc]);\n",
    "\n",
    "def trajc12(n10,n20,c10,c20,d,R, RA, tmax=1000, nstep=10000):\n",
    "    t = np.linspace(0,tmax,nstep); dt=t[1]-t[0];\n",
    "    nc0=np.array([n10,n20,c10,c20]);\n",
    "    nc = integrate.odeint(dnc2, nc0, t, args= (d,R,RA,c10,c20) );\n",
    "    f = nc[-1,0]/(nc[-1,0]+nc[-1,1]); c=(f,1-f,0);\n",
    "    return nc,c\n",
    "def trajc123(n10,n20,n30,c10,c20,d,R, RA, tmax=1000, nstep=10000):\n",
    "    t = np.linspace(0,tmax,nstep); dt=t[1]-t[0];\n",
    "    nc0=np.array([n10,n20,n30,c10,c20]);\n",
    "    nc = integrate.odeint(dnc3, nc0, t, args= (d,R,RA,c10,c20) );\n",
    "    f1 = nc[-1,0]/(nc[-1,0]+nc[-1,1]+nc[-1,2]);f2 = nc[-1,1]/(nc[-1,0]+nc[-1,1]+nc[-1,2]);f3 = nc[-1,2]/(nc[-1,0]+nc[-1,1]+nc[-1,2]);\n",
    "    if(f1<1/nstep): f1=0;\n",
    "    if(f2<1/nstep): f2=0;\n",
    "    if(f3<1/nstep): f3=0;\n",
    "    if(f1>1): f1=1;\n",
    "    if(f2>1): f2=1;\n",
    "    if(f3>1): f3=1;\n",
    "    c=(f1,f2,f3);\n",
    "    return nc,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast(growth):\n",
    "    # growth: nsim x S x R\n",
    "    # return order of growth rates (i.e. [0,1,2])\n",
    "    # np.argsort (sum of growth rates on all resources for each species)\n",
    "    # then flip to get the maximum in the first\n",
    "    nsim, S, R = growth.shape;\n",
    "    return np.argsort( np.sum(growth, axis=-1), axis=-1 )\n",
    "def pred_growth(surv, growth, thres=1e-2):\n",
    "    surv = surv>thres;\n",
    "    order = fast(growth);\n",
    "    winner = order[:,-1];\n",
    "    nsim, S = order.shape;\n",
    "    pred = np.zeros((nsim, S));\n",
    "    pred[ np.arange(nsim), winner ] = 1;\n",
    "    prediction = np.zeros(nsim);\n",
    "    prediction = np.sum( ((surv[:,-1] - pred)**2)/S, axis=-1 );\n",
    "    return 1-prediction    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_surv(surv, thres=2e-5):\n",
    "    # filter out extinction cases\n",
    "    nsim, dumi, S = surv.shape;\n",
    "    cnt = np.ones(nsim);\n",
    "    for i in np.arange(nsim):\n",
    "        for j in np.arange(dumi):\n",
    "            if(np.sum(surv[i,j] > thres) == 0):\n",
    "                cnt[i] = 0;\n",
    "    return cnt==1;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
